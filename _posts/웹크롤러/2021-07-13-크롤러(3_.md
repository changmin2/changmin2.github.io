---
title:  "웹크롤러(크롤링 코드)"

categories:
  - 웹크롤러
tags:
  - Blog
toc: true
toc_sticky: true
---

## 실전 코드

지금 설명하는 코드는 청와대 국민청원 게시판 크롤링하는 예제입니다.

```python

    #Step 1. 필요한 모듈을 로딩합니다
    from selenium import webdriver
    import time 
    import math
    import pandas as pd
    from bs4 import BeautifulSoup
    print("청와대 국민청원 게시판 크롤링")

    #Step 3. 수집된 데이터를 저장할 파일 이름 입력받기 
    f_dir = input("2.파일을 저장할 폴더명만 쓰세요(기본값:C:\):")
    if f_dir == '' :
        f_dir="C:\"
        
    collect_cnt = int(input('몇 건을 수집하시겠습니까?: '))
    collect_page_cnt = math.ceil(collect_cnt / 7) #전체게시판에 나타나는 글 수가 총 7개이므로 7로 나눠준다.

    #Step 4. 크롬 드라이버 설정 및 웹 페이지 열기
    chrome_path = "C:\chromedriver\chromedriver.exe"
    driver = webdriver.Chrome(chrome_path)

    url = 'http://www1.president.go.kr/petitions'
    driver.get(url)
    time.sleep(2) # 창이 켜지는걸 기다려주기 위함
    driver.maximize_window() # 화면 최대화


    #Step 9. 데이터 수집하기
    no2=[]           # 게시글 번호 컬럼
    title2=[ ]       # 게시글 제목 컬럼
    join2=[]       # 논문 저자 컬럼
    category2=[ ]     # 소속 기관 컬럼
    start2=[ ]        # 게시글 날짜 컬럼
    end2=[ ]       # 국내석사 컬럼
    contents2=[]     # 초록내용

    no = 1           # 게시글 번호 초기값
    check=[] #참여인원 임시저장
    # 다음 페이지 번호 만들기
    page_no=[]
    getget=[] #게시글제목 임시저장
    bb=[]      #청원번호임시저장     
    for a in range(1,collect_page_cnt+1) :
        print("\n")
        print("%s 번째 국민청원 게시글 상세 정보입니다.=======================" %a)
        

        html = driver.page_source #페이지 정보를 다 불러온다.
        soup = BeautifulSoup(html, 'html.parser')
        content_list = soup.find('div','ct_list1').find_all('li')

        for i in content_list:

            # 게시글 제목 체크하기
            try:
                title=i.find('div','bl_subject').get_text().strip()
                getget=title.split(' ') # [제목 ,번호 ] 로 리스트에 저장된다.
                
            except :
                continue 
            else :
                # 1.청원 번호
                number = i.find('div','bl_no').get_text() #[번호, 숫자]로 리스트에 저장된다.
                bb=number.split(' ')
                no2.append(bb[1])
                print('1.청원번호:',bb[1])
                
                # 2. 논문 제목
                title2.append(getget[1].strip())
                print("2.제목 : %s" %getget[1].strip())

                # 3. 참여인원
                join=i.find('div','bl_agree cs').get_text()
                
                check = join.split(' ')
                print('3.참여인원:',check[1])
                join2.append(check[1])

                # 4. 카테고리
                company=i.find('div','bl_category ccategory cs wv_category').get_text()#[분류, 분류내용]로 저장된다.
                category = company.split(' ')
                category2.append(category[1])
                print('4.카테고리:',category[1])

                url_1 = i.find('div','bl_subject').find('a')['href'] #[href]로 지정하면 url을 가져온다.
                full_url = 'http://www1.president.go.kr'+url_1

                time.sleep(1)

                driver.get(full_url) # 클릭한 url의 페이지를 가져온다.

                html_1 = driver.page_source
                soup_1 = BeautifulSoup(html_1, 'html.parser')  
                #청원시작일
                start = soup_1.find('div','petitionsView_info').get_text()
                y = start.split()
                start2.append(y[1][4:])
                print('5.청원시작일:', y[1][4:])

                #청원종료일
                end2.append(y[2][4:])
                print('5.청원종료일:', y[2][4:])
                #청원 내용
                con = soup_1.find('div','View_write').get_text().strip()
                contents2.append(con)
                print('6.청원내용:',con)
                time.sleep(1)

                driver.back()  # 이전 페이지로 돌아가기



                time.sleep(2)

                no += 1
                
                if no>collect_cnt : # 지정한 건수를 넘어서면 break를 걸어서 종료한다.
                    break
                                
        a += 1 #다음페이지번호를 카운트함
        
        try :
                driver.find_element_by_link_text('%s' %a).click() # 다음 페이지번호 클릭
        except :
                driver.find_element_by_link_text('다음 페이지로').click()
            
        time.sleep(2)
        
    print("요청하신 작업이 모두 완료되었습니다")

    # Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기
    # 현재 날짜와 시간으로 폴더 만들고 파일 이름 설정하기
    import time
    import os

    # # 데이터 프레임 생성 후 xls , csv 형식으로 저장하기
    # import pandas as pd 

    df = pd.DataFrame()
    df['번호']=no2
    df['제목']=pd.Series(title2)
    df['참여인원']=pd.Series(join2)
    df['카테고리']=pd.Series(category2)
    df['청원시작일']=pd.Series(start2)
    df['청언종료일']=pd.Series(end2)
    df['청원내용']=pd.Series(contents2)


    # xls 형태로 저장하기
    df.to_excel(f_dir,index=False, encoding="utf-8")


    print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')

```