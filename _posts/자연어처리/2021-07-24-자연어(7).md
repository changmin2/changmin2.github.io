---
title:  "자연어처리(모델 생성 및 검증)"

categories:
  - 자연어
tags:
  - Blog
toc: true
toc_sticky: true
---

## 모델 생성 및 검증

### Train 데이터와 Validation 데이터 준비

```python
    # 데이터 크기 예) (25000,10000), 레이블 크기 예) (25000,10000)
    indices = np.arange(data.shape[0]) #0~24999까지의 숫자를 생성
    np.random.shuffle(indices) # 숫자를 랜덤하게 섞음
    data = data[indices] # 이것을 인덱스로 하여 데이터를 섞음
    labels = labels[indices] # 라벨도 같이 섞음
```

### 훈련데이터와 검증데이터 분리

```python
    x_train = data[validation_ratio:] # 훈련데이터의 70%를 훈련데이터
    y_train = labels[validation_ratio:] # 훈련데이터의 %를 훈련데이터 Label (data와 labels는 같은 순서)
    x_val = data[:validation_ratio] # 훈련데이터의 30%를 검증데이터
    y_val = labels[:validation_ratio] # 훈련데이터의 30%를 검증데이터 Label
```

### 모델 정의하기

```python
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    model = Sequential() # 모델을 새로 정의
    model.add(Dense(64, activation='relu', input_shape=(max_words,))) # 첫 번째 은닉층. activation은 다음 층으로 값을 넘기는 방법
    model.add(Dense(32, activation='relu')) # 두 번째 은닉층
    model.add(Dense(1, activation='sigmoid')) # 출력층 
```
모델 요약 출력하는 함수 : model.summary()

### Complie & Train Model

```python
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])
    # 32개의 미니배치를 만들어 10번의 epochs로 훈련한다. 나중에 제일 좋은 epochs값을 찾아서 그 값을 넣어서 다시 실행한다.
    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))
```

### Save Model

```python

    model.save('text_binary_model.h5')
    import pickle
    with open('text_binary_tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    # dump(객체,파일)로 미리 선언한 tokenizer객체로 10,000개의 단어로 된 Tokenizer를 저장한다.
```

### Accuracy & Loss 확인

```python
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    print('Accuracy of each epoch:', acc) # [0.79, 0.90, 0.93, 0.94, 0.96, 0.97, 0.98, 0.98, 0.98, 0.99]
    epochs = range(1, len(acc) +1)
```

### Data Sequencing

```python
    # 문자열을 tokenizer로 숫자 리스트로 변환
    data = loaded_tokenizer.texts_to_sequences(texts)
    # padding으로 문자열의 길이를 고정시킨다
    data = pad_sequences(data, maxlen=maxlen) 
    # test 데이터를 원-핫 인코딩한다
    x_test = to_one_hot(data, dimension=max_words)
    # label을 list에서 넘파이 배열로 변환. 결과가 0 또는 1만 나오므로 이와같이 int32로 저장해도 된다
    # 다중분류에서는 이 부분도 원-핫 인코딩 한다
    y_test = np.asarray(labels)
```

### Test Data Evaluation

```python
    test_eval = loaded_model.evaluate(x_test, y_test) # 모델에 분류할 데이터와 그 정답을 같이 넣어준다
    print('prediction model loss & acc:', test_eval) 
    # 손실값 1.06, 정확도 84.9% Accuracy & Loss 에서 확인한 epochs값을 넣으주면 정확도가 올라간다.
```

