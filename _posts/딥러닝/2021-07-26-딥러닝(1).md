---
title:  "딥러닝(기초)"

categories:
  - 딥러닝
tags:
  - Blog
toc: true
toc_sticky: true
---

## 딥러닝 기초

- optimizer 종류

otms=optimizers.SGD(learning_rate=0.01)
otms=optimizers.Adagrad(learning_rate=0.01) 
otms=optimizers.Adamax(learning_rate=0.01) 
otms=optimizers.Ftrl(learning_rate=0.01)
otms=optimizers.Nadam(learning_rate=0.01)
otms=optimizers.RMSprop(learning_rate=0.01)


- 기본 모델 설정하기

1. import할 모듈들

    ```python
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras import optimizers
    import numpy as np
    ```
2. 모델 빌드하기

    ```python
      def build_model():
        model = Sequential()
        model.add(Dense(1,input_dim=train_data.shape[1],activation='linear'))
        model.compile(optimizer='nadam',loss='mse',metrics=['accuracy'])
        return model
    ```

3. 검증에 사용할 k-fold 교차 모델

    ```python
    k = 4
    num_val_samples = len(train_data) // k
    num_epochs = 100
    all_scores = []

    for i in range(k):
        print('처리중인 폴드: #', i)
        val_data = train_data[i * num_val_samples:(i+1)*num_val_samples] #싸이킷런으로 분리하고 돌려도 되긴 함
        val_target = train_labels[i * num_val_samples:(i+1)*num_val_samples]
        ##왜 교차검증을 해야할까? 
        
        partial_train_data = np.concatenate(
            [train_data[:i*num_val_samples],
            train_data[(i+1) * num_val_samples:]], axis=0)
        
        partial_train_target = np.concatenate(
            [train_labels[:i*num_val_samples],
            train_labels[(i+1) * num_val_samples:]], axis=0)
        
        model = build_model()
        model.fit(partial_train_data, partial_train_target, epochs=num_epochs,batch_size=10)
        
        val_mse, accuracy =  model.evaluate(val_data, val_target)
        all_scores.append(val_mse)
        print('all_scores:', all_scores)
    ```

### 딥러닝 알고리즘들

1. 로지스틱 회귀

    ```python
    x = np.array([-30,-25,-20,-15,-5,0,5,10,15,20,25,30])
    y = np.array([0,0,0,0,0,1,1,1,1,1,1,1])
    model = build_model() # sequential 모델 생성
    model.add(Dense(1,input_dim=1,activation='sigmoid')) # 리니어리그레이션에 의해 하나의 결과값(시그모이드로인해)만 나오고 하나만들어간다. 
    # 입력층에서는 값이 여러개 존재하겠지만 지금 가정은 은닉층이므로 시그모이드로인해 출력값하나 입력값하나이다. 딥러닝에서는 시그모이드와 리니어리그레이션이랑 비슷한말이다
    opt = optimizers.Adam(learning_rate=0.01)
    model.compile(optimizer=opt, loss='binary_crossentropy',metrics=['binary_accuracy'])# 이진분류이므로 loss를 다르게설정, 정확도도 이진이므로 다르게설정
    model.fit(x, y, epochs=num_epochs,batch_size=10,shuffle=False)
    ```

2. l2 알고리즘

    ```python
      from tensorflow.keras import regularizers
      model = Sequential()
      model.add(Dense(1,input_dim=13,activation='linear',kernel_regularizer = regularizers.l2(0.001)))

    ```

3. feature 스케일링

표준화 코드이다. 이것말고도 minmax스케일러로 정규화 가능

```python
  mean = train_data.mean(axis=0)
  train_data-=mean
  std = train_data.std(axis=0)
  train_data/=std

  test_data -=mean # 항상 입력데이터 기준이라서 입력데이터의 평균을빼야함
  test_data/=std
```

